Arguments: Namespace(local_rank=None, use_sync_bn=False, use_amp=False, script_mode='train', config='/content/drive/MyDrive/SemNov_AML_DAAI_23-24/cfgs/Pointbertb32.yaml', seed=1, epochs=250, batch_size=64, num_workers=4, resume=None, apply_fix_cellphone=True, data_root='./3D_OS_release_data', checkpoints_dir='outputs', exp_name='OpenShape_Pointbertb32_SR2', eval_step=5, save_step=10, ckpt_path=None, src='SR2', sonn_split='main_split', sonn_h5_name='objectdataset.h5', augm_set='rw', grad_norm_clip=-1, num_points=1024, num_points_test=2048, wandb_name=None, wandb_group='md-2-sonn-augmCorr', wandb_proj='AML_DAAI_proj23_24_test', loss='CE', cs=False, cs_gan_lr=0.0002, cs_beta=0.1, save_feats=None, fine_tuning='/content/drive/MyDrive/openshape-pointbert-vitb32-rgb/model.pt', corruption=None, tar1='none', tar2='none', log_dir='outputs/OpenShape_Pointbertb32_SR2', tb_dir='outputs/OpenShape_Pointbertb32_SR2/tb-logs', models_dir='outputs/OpenShape_Pointbertb32_SR2/models', backup_dir='outputs/OpenShape_Pointbertb32_SR2/backup-code')
Config: {'optimizer': {'type': 'sgd', 'skip_wd': [], 'weight_decay': 0.0001, 'kwargs': {'lr': 0.1, 'momentum': 0.9}}, 'scheduler': {'type': 'CosLR', 'kwargs': {'t_initial': 250, 'cycle_limit': 1, 'lr_min': 0.0001}}, 'model': {'ENCO_NAME': 'PointBERTB32', 'cla_input_dim': 512, 'dropout': 0.5, 'act': 'gelu'}}
World size: 1

SR2 train synset: {'bed': 0, 'toilet': 1, 'desk': 2, 'monitor': 3, 'table': 2}
Source: SR2
Num training classes: 4
Model: 
Classifier(
  (enco): PointPatchTransformer(
    (sa): PointNetSetAbstraction(
      (mlp_convs): ModuleList(
        (0): Conv2d(9, 64, kernel_size=(1, 1), stride=(1, 1))
        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (mlp_bns): ModuleList(
        (0-1): 2 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (lift): Sequential(
      (0): Conv1d(131, 512, kernel_size=(1,), stride=(1,))
      (1): Lambda()
      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (transformer): Transformer(
      (layers): ModuleList(
        (0-11): 12 x ModuleList(
          (0): PreNorm(
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (attend): Softmax(dim=-1)
              (dropout): Dropout(p=0.0, inplace=False)
              (to_qkv): Linear(in_features=512, out_features=1536, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): PreNorm(
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=512, out_features=1024, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=1024, out_features=512, bias=True)
                (4): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
    )
  )
  (penultimate): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=False)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU(approximate='none')
    (3): Dropout(p=0.5, inplace=False)
    (4): Linear(in_features=512, out_features=256, bias=False)
  )
  (head): Sequential(
    (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=256, out_features=4, bias=True)
  )
)

param count: 
25.6935 M
Loss: CE

it: [10/29-1/250], rank: [1/1], Loss: 0.0747, Loss avg: 0.4625, lr: 0.100000, BT: 0.35, DT: 0.01
it: [20/29-1/250], rank: [1/1], Loss: 0.0411, Loss avg: 0.2686, lr: 0.100000, BT: 0.39, DT: 0.01
Train [1/250]	rank: [1/1], Loss: 0.2057, Acc: 0.9273, Bal Acc: 0.9259, BT: 0.53, DT: 0.03,  epoch time: 15.43
it: [10/29-2/250], rank: [1/1], Loss: 0.0022, Loss avg: 0.0590, lr: 0.099996, BT: 0.34, DT: 0.00
it: [20/29-2/250], rank: [1/1], Loss: 0.0379, Loss avg: 0.0847, lr: 0.099996, BT: 0.37, DT: 0.00
Train [2/250]	rank: [1/1], Loss: 0.0758, Acc: 0.9822, Bal Acc: 0.9818, BT: 0.37, DT: 0.02,  epoch time: 10.85
it: [10/29-3/250], rank: [1/1], Loss: 0.0101, Loss avg: 0.0291, lr: 0.099984, BT: 0.35, DT: 0.00
it: [20/29-3/250], rank: [1/1], Loss: 0.1789, Loss avg: 0.0460, lr: 0.099984, BT: 0.35, DT: 0.00
Train [3/250]	rank: [1/1], Loss: 0.0481, Acc: 0.9833, Bal Acc: 0.9837, BT: 0.38, DT: 0.03,  epoch time: 11.02
it: [10/29-4/250], rank: [1/1], Loss: 0.0075, Loss avg: 0.0280, lr: 0.099965, BT: 0.38, DT: 0.01
it: [20/29-4/250], rank: [1/1], Loss: 0.0015, Loss avg: 0.0352, lr: 0.099965, BT: 0.41, DT: 0.01
Train [4/250]	rank: [1/1], Loss: 0.0325, Acc: 0.9903, Bal Acc: 0.9905, BT: 0.43, DT: 0.04,  epoch time: 12.45
it: [10/29-5/250], rank: [1/1], Loss: 0.0051, Loss avg: 0.0280, lr: 0.099937, BT: 0.35, DT: 0.00
it: [20/29-5/250], rank: [1/1], Loss: 0.0055, Loss avg: 0.0231, lr: 0.099937, BT: 0.35, DT: 0.00
Train [5/250]	rank: [1/1], Loss: 0.0345, Acc: 0.9881, Bal Acc: 0.9872, BT: 0.38, DT: 0.02,  epoch time: 11.27
Test [5/250]	Acc: 1.0000, Bal Acc: 1.0000
it: [10/29-6/250], rank: [1/1], Loss: 0.0175, Loss avg: 0.0133, lr: 0.099901, BT: 0.36, DT: 0.00
it: [20/29-6/250], rank: [1/1], Loss: 0.0037, Loss avg: 0.0311, lr: 0.099901, BT: 0.37, DT: 0.00
Train [6/250]	rank: [1/1], Loss: 0.0280, Acc: 0.9914, Bal Acc: 0.9917, BT: 0.39, DT: 0.02,  epoch time: 11.33
it: [10/29-7/250], rank: [1/1], Loss: 0.0049, Loss avg: 0.0395, lr: 0.099858, BT: 0.36, DT: 0.00
it: [20/29-7/250], rank: [1/1], Loss: 0.0155, Loss avg: 0.0343, lr: 0.099858, BT: 0.36, DT: 0.00
Train [7/250]	rank: [1/1], Loss: 0.0286, Acc: 0.9908, Bal Acc: 0.9907, BT: 0.40, DT: 0.03,  epoch time: 11.57
it: [10/29-8/250], rank: [1/1], Loss: 0.0056, Loss avg: 0.0331, lr: 0.099807, BT: 0.40, DT: 0.01
it: [20/29-8/250], rank: [1/1], Loss: 0.0566, Loss avg: 0.0255, lr: 0.099807, BT: 0.37, DT: 0.00
Train [8/250]	rank: [1/1], Loss: 0.0264, Acc: 0.9903, Bal Acc: 0.9901, BT: 0.42, DT: 0.04,  epoch time: 12.27
it: [10/29-9/250], rank: [1/1], Loss: 0.0208, Loss avg: 0.0119, lr: 0.099748, BT: 0.40, DT: 0.01
it: [20/29-9/250], rank: [1/1], Loss: 0.0015, Loss avg: 0.0250, lr: 0.099748, BT: 0.39, DT: 0.01
Train [9/250]	rank: [1/1], Loss: 0.0332, Acc: 0.9887, Bal Acc: 0.9888, BT: 0.41, DT: 0.03,  epoch time: 12.04
it: [10/29-10/250], rank: [1/1], Loss: 0.0119, Loss avg: 0.0248, lr: 0.099681, BT: 0.38, DT: 0.00
it: [20/29-10/250], rank: [1/1], Loss: 0.1601, Loss avg: 0.0262, lr: 0.099681, BT: 0.39, DT: 0.00
Train [10/250]	rank: [1/1], Loss: 0.0225, Acc: 0.9925, Bal Acc: 0.9926, BT: 0.41, DT: 0.02,  epoch time: 11.94
Test [10/250]	Acc: 1.0000, Bal Acc: 1.0000
it: [10/29-11/250], rank: [1/1], Loss: 0.0061, Loss avg: 0.0225, lr: 0.099606, BT: 0.39, DT: 0.00
it: [20/29-11/250], rank: [1/1], Loss: 0.0354, Loss avg: 0.0220, lr: 0.099606, BT: 0.41, DT: 0.00
Train [11/250]	rank: [1/1], Loss: 0.0211, Acc: 0.9941, Bal Acc: 0.9940, BT: 0.42, DT: 0.03,  epoch time: 12.35
